<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Twitter Mining</title>
	<link rel="stylesheet" type="text/css" href="main.css">
</head>

<body>
	<header>
		<h1>Twitter Mining and Analysis on Influential Twitter Users and Africa Government Officials</h1>
	</header>
	<section class="container">
		<div class="blurb">
			<h2>Introduction</h2>
			<p>
				Social media campaigns
				are mostly driven by influencial individuals in the network and therefore identifying them becomes such a big
				win.
				This project was undertaken to find out the most influential twitter users, which hashtags are
				used, and what parameters are used to identify influential rate of a twitter user. 
				Parameters used for the task were: 
				<ul>
					<li>Popularity score: measured by the number of retweets and likes users get</li>
					<li>Reach score: measured by the size of audience a single user has (comprise of followers and the people they follow)</li>
					<li>Relevance score: measured by the number of mentions a single user has.</li>
				</ul>
			</p>
			<h2>Methodology</h2>
			<h3>Data Extraction</h3>
			<p>
				Data extraction is the process of getting access to data from its source. The source can be different formats like databases,
				websites, APIs, electronic devices, forms and many more. In this case I extracted twitter handles for both Africa goverment officials
				and twitter influencers from the following websites respectively: 
				<a href="https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-ontwitter/#east-africa">Africa
				leaders response to Corona virus</a> websites.
				<a href="https://africafreak.com/100-most-influential-twitter-users-in-africa">Africa Influential TweeterUsers</a>
				I used beautiful soup for scraping the websites. Beautiful soup is a Python third party library for parsing HTML and XML 
				documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, also known as web scraping.  
				One advantage of beautiful soup is its ability scrape websites that have mulformed markup, i.e., non-closed tags. More information on 
				beautiful soup can be found on its <a href="https://pypi.org/project/beautifulsoup4/">documentation page</a>
				Other packages used in this project include pandas, matplotlib and seaborn. These packages comes pre-installed when using jupyter notebook.
				
				<br>The code below shows the data extraction part: <br><br>
				<code>
					# Imports <br>
					from requests import get <br>
					from requests.exceptions import RequestException <br>
					from contextlib import closing <br>
					from bs4 import BeautifulSoup <br>
					import pandas as pd <br>
					import matplotlib.pyplot as plt <br>
					
					import os, sys <br>
					import requests <br>
					import urllib.request <br>
					import time <br>
					import fire <br>
					import re <br>
					
				</code>
				<code>
				# functions for data extraction <br>
				def simple_get(url): <br>
				    """ <br>
				    Attempts to get the content at `url` by making an HTTP GET request. <br>
				    If the content-type of response is some kind of HTML/XML, return the <br>
				    text content, otherwise return None. <br>
				    """ <br>
				    try: <br>
					with closing(get(url, stream=True)) as resp: <br>
					    if is_good_response(resp): <br>
						return resp.content  #.encode(BeautifulSoup.original_encoding) <br>
					    else: <br>
						return None <br>
				    except RequestException as e: <br>
					log_error('Error during requests to {0} : {1}'.format(url, str(e))) <br>
					return None <br>
				def is_good_response(resp): <br>
				    """ <br>
				    Returns True if the response seems to be HTML, False otherwise. <br>
				    """ <br>
				    content_type = resp.headers['Content-Type'].lower() <br>
				    return (resp.status_code == 200 <br>
					    and content_type is not None <br>
					    and content_type.find('html') > -1) <br>
				</code>
				<code>
				def log_error(e): <br>
				    """ <br>
				    It is always a good idea to log errors. <br>
				    This function just prints them, but you can <br>
				    make it do anything. <br>
				    """ <br>
				    print(e) <br>
				</code>
				<code>
				def get_elements(url, tag='',search={}, fname=None): <br>
				    """ <br>
				    Downloads a page specified by the url parameter <br>
				    and returns a list of strings, one per tag element <br>
				    """ <br>

				    if isinstance(url,str): <br>
					response = simple_get(url) <br>
				    else: <br>
					#if already it is a loaded html page <br>
					response = url <br>

				    if response is not None: <br>
					html = BeautifulSoup(response, 'html.parser') <br>

					res = [] <br>
					if tag:    <br>
					    for li in html.select(tag): <br>
						for name in li.text.split('\n'): <br>
						    if len(name) > 0: <br>
							res.append(name.strip()) <br>
					if search: <br>
					    soup = html <br>          
					    r = '' <br>
					    if 'find' in search.keys(): <br>
						print('findaing',search['find']) <br>
						soup = soup.find(**search['find']) <br>
						r = soup <br>
					
					    if 'find_all' in search.keys(): <br>
						print('finding all of',search['find_all']) <br>
						r = soup.find_all(**search['find_all']) <br>

					    if r: <br>
						for x in list(r): <br>
						    if len(x) > 0: <br>
							res.extend(x) <br>
					return res <br>
				    # Raise an exception if we failed to get any data from the url <br>
				    raise Exception('Error retrieving contents at {}'.format(url))  <br>  
				if get_ipython().__class__.__name__ == '__main__': <br>
				</code>
				<code>		
				# scraping data for the influential twitter users in Africa <br>
				url = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa',tag='h2') <br>
				</code>
				<code>
				# saving the twitter handles to a dataframe and converting to a list <br>
				df = pd.DataFrame(url).head(100) <br>
				#Data manipulation <br>
				df = new[0].str.split('.', expand=True) <br>
				df.head(100) <br>
				df = df1[1].str.split('(', expand=True) <br>
				df.head(100) <br>
				df[1] = df2[1].str.strip(')') <br>
				df.head(100) <br>
				df.columns = ['Twitter_name','Twitter_handle'] <br>

				twitter_handle = df["Twitter_handle"].astype(str).tolist() <br>

				twitter_handle
				</code>
				
				<p>The twitter handles for influential twitter users should be printed when the list is called.</p>
		
				<p>We repeat the scraping process for Africa government officials</p> <br>
		
		                <code>
				# scraping the data Africa government officials <br>
				url = get_elements('https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa')<br>
				response = simple_get(url) <br>
				response = get_elements(response, search={'find_all':{'class_':'twitter-tweet'}})<br>
				twitter_handles_gov = []<br>
				for tag in response:<br>
				    if tag.string != None:<br>
					twitter_handles_gov.append(tag.string)<br>
					
				# getting clean twitter handles<br>
				my_regex = re.compile(r'@[a-zA-Z0-9_]{0,15}')<br>
				tags = ''.join(mystrings)<br>
				twitter_handles = my_regex.findall(tags)<br>
				twitter_handles = pd.DataFrame(twitter_handles)<br>

				twitter_handles.columns = ["Twitter_handle"]<br>

				twitter_handles_gov = twitter_handles["Twitter_handles"].astype(str).to_list()<br>
		                </code>
				<p>By now both the list for goverment twitter handles and influential twitter users should be ready </p>
				<p>The next step is to scrape twitter information for the two lists of the twitter handles. 
				To do this, we will need a very handy library called tweepy. For more tweepy information and installation process,
				read the documentation <a href="http://docs.tweepy.org/en/latest/">here</a>
				
				The code below scrapes the twitter data and does some cleaning on the user's text. <br> 
				<code>
				from __future__ import unicode_literals<br>
				import sys<br>
				import os<br>
				import json <br>
				import re <br>
				import string<br>

				import matplotlib.dates as mdates<br>


				# to view all columns<br>
				pd.set_option("display.max.columns", None)<br>

				# Import the necessary methods from tweepy library<br>

				import tweepy<br>
				from tweepy.streaming import StreamListener<br>
				from tweepy import OAuthHandler<br>
				from tweepy import Stream<br>

				# Sentiment analysis package<br>
				from textblob import TextBlob

				# general text pre-processor<br>

				import nltk<br>
				from nltk.corpus import stopwords<br>
				#nltk.download('punkt')<br>

				# tweet pre-processor<br>
				import preprocessor as p<br>

				'''<br>
				def print_full(x):<br>
				     # This is to print nicely DataFrame wide tables<br>
				    pd.set_option('display.max_rows', len(x))<br>
				    pd.set_option('display')'<br>

				'''<br>

				class tweetsearch():<br>

				    def __init__(self, cols=None, auth=None):<br>

					if not cols is None:<br>
					    self.cols = cols<br>
					else:<br>
					    self.cols = [<br>
						'id', 'created_at', 'source', 'original_text', 'clean_text',<br>
						'sentiment', 'polarity', 'subjectivity','lang','favorite_count', <br>
						'retweet_count','original_author', 'hashtgs','user_mentions',<br>
						'followers', 'following', 'place_coordinates', 'place'<br>
					    ]<br>

					if auth is None: <br>            

					    consumer_key = os.environ.get('TWITTER_API_KEY')<br>
					    consumer_secret = os.environ.get('TWITTER_API_SECRET')<br>
					    access_token = os.environ.get('TWITTER_ACCESS_TOKEN')<br>
					    access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')<br>

					    # This handles Twitter authentication <br>
					    auth = OAuthHandler(consumer_key, consumer_secret)<br>
					    auth.set_access_token(access_token, access_token_secret)<br>

					self.auth = auth <br>
					self.api = tweepy.API(auth, wait_on_rate_limit=True) <br>
					self.filtered_tweet = '' <br>

				    def clean_tweets(self, twitter_text):<br>

					# use pre processor <br>
					tweet = p.clean(twitter_text)<br>

					#Happy Emoticons <br>
					emoticons_happy = set([ <br>
					    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', <br>
					    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', <br>
					    '=-3', '=3', ':-))', ":'-)", ":')", ':*', ':^*', '>:P', ':-P', ':P', 'X-P', <br>
					    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', <br>
					    '<3' <br>
					    ]) <br>

					# Sad Emoticons <br>
					emoticons_sad = set([ <br>
					    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', <br>
					    ':-[', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'-(", ":'(", ':\\', ':-c', <br>
					    ':c', ':{', '>:\\', ';(' <br>
					    ]) <br>

					#Emoji patterns <br>
					emoji_pattern = re.compile("[" <br>
						 u"\U0001F600-\U0001F64F"  # emoticons <br>
						 u"\U0001F300-\U0001F5FF"  # symbols & pictographs <br>
						 u"\U0001F680-\U0001F6FF"  # transport & map symbols <br>
						 u"\U0001F1E0-\U0001F1FF"  # flags (iOS) <br>
						 u"\U00002702-\U000027B0" <br>
						 u"\U000024C2-\U0001F251" <br>
						 "]+", flags=re.UNICODE) <br>

					# combine sad and happy emoticons <br>
					emoticons = emoticons_happy.union(emoticons_sad) <br>

					stop_words = set(stopwords.words('english')) <br>
					word_tokens = nltk.word_tokenize(tweet) <br>

					tweet = re.sub(r':', '', tweet) <br>
					tweet = re.sub(r'‚Ä¶', '', tweet) <br>

					#replace consecutive non-ASCII characters with a space <br>
					tweet = re.sub(r'[^\x00-\x7F]+',' ', tweet) <br>

					#remove emojis from tweet <br>
					tweet = emoji_pattern.sub(r'', tweet) <br>

					#filter using NLTK library append it to a string <br>
					filtered_tweet = [w for w in word_tokens if not w in stop_words] <br>

					# looping through conditions <br>
					filtered_tweet = [] <br>
					for w in word_tokens: <br>
					    # check tokens againts stop words, emoticons and punctuations <br>
					    if w not in stop_words and w not in emoticons and w not in string.punctuation: <br>
						filtered_tweet.append(w) <br>

					return ' '.join(filtered_tweet) <br>

				    def get_tweets(self, name, csvfile=None): <br>

					df = pd.DataFrame(columns=self.cols) <br>

					if not csvfile is None: <br>
					    # If the file exists, then read the existing data from the CSV file <br>
					    if os.path.exists(csvfile): <br>
						df = pd.read_csv(csvfile, header=0) <br>



					# page attribute in tweepy.cursor and iteration <br>
					for page in tweepy.Cursor(self.api.user_timeline, id=name).pages(1): <br>

					    for status in page: <br>
						new_entry = [] <br>
						status = status._json <br>

						# filter by language <br>
						#if status['lang'] != 'en': <br>
						 #   continue <br>

						# if this tweet is a retweet update retweet count <br>
						if status['created_at'] in df['created_at'].values: <br>
						    i = df.loc[df['created_at'] == status['created_at']].index[0] <br>

						    cond1 = status['favorite_count'] != df.at[i, 'favorite_count'] <br>
						    cond2 = status['retweet_count'] != df.at[i, 'retweet_count'] <br>
						    if cond1 or cond2: <br>
							df.at[i, 'favorite_count'] = status['favorite_count'] <br>
							df.at[i, 'retweet_count'] = status['retweet_count'] <br>
						    continue <br>

						# calculate sentiment <br>
						filtered_tweet = self.clean_tweets(status['text']) <br>
						blob = TextBlob(filtered_tweet) <br>
						Sentiment = blob.sentiment <br>
						polarity = Sentiment.polarity <br>
						subjectivity = Sentiment.subjectivity <br>

						new_entry += [status['id'], status['created_at'], <br>
							      status['source'], status['text'], filtered_tweet, <br>
							      Sentiment, polarity, subjectivity, status['lang'], <br>
							      status['favorite_count'], status['retweet_count']] <br>


						new_entry.append(status['user']['screen_name']) <br>

						hashtags = ", ".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']]) <br>
						new_entry.append(hashtags) # append the hashtags <br>

						mentions = ", ".join([mention['screen_name'] for mention in status['entities']['user_mentions']]) <br>
						new_entry.append(mentions) # append user mentions <br>

						try: <br>
						    followers = status['user']['followers_count'] <br>
						except TypeError: <br>
						    followers = None <br>

						new_entry.append(followers) <br>

						try: <br>
						    following = status['user']['friends_count'] <br>
						except TypeError: <br>
						    following = None <br>

						new_entry.append(following) <br>


						try: <br>
						    xyz = status['place']['bounding_box']['coordinates'] <br>
						    coordinates = [coord for loc in xyz for coord in loc] <br>
						except TypeError: <br>
						    coordinates = None <br>

						new_entry.append(coordinates) <br>

						try: <br>
						    location = status['user']['location'] <br>
						except TypeError: <br>
						    location = '' <br>

						new_entry.append(location) <br>

						# appending a row to the data frame <br>
						single_user_df = pd.DataFrame([new_entry], columns=self.cols) <br>
						df = df.append(single_user_df, ignore_index=True) <br>

					if not csvfile is None: <br>
					    df.to_csv(csvfile, columns=self.cols, index=False, encoding="utf-8") <br>

					return df <br>
				</code>
				
				Since we now have a twitter data in dataframes, we can do some data processing in the next step.
				<h3>Data Cleaning</h3>
				<p>
					Since the data was not all clean, some of the cleaning tasks like removing emojis and non-text symbols was necessary.
					The code below did the functionality <br>
				</p>
				<code>
				 def clean_tweets(self, twitter_text): <br>

					# use pre processor <br>
					tweet = p.clean(twitter_text) <br>

					#Happy Emoticons <br>
					emoticons_happy = set([ <br>
					    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', <br>
					    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', <br>
					    '=-3', '=3', ':-))', ":'-)", ":')", ':*', ':^*', '>:P', ':-P', ':P', 'X-P', <br>
					    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', <br>
					    '<3' <br>
					    ])

					# Sad Emoticons <br>
					emoticons_sad = set([ <br>
					    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', <br>
					    ':-[', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'-(", ":'(", ':\\', ':-c', <br>
					    ':c', ':{', '>:\\', ';(' <br>
					    ]) <br>

					#Emoji patterns <br>
					emoji_pattern = re.compile("[" <br>
						 u"\U0001F600-\U0001F64F"  # emoticons <br>
						 u"\U0001F300-\U0001F5FF"  # symbols & pictographs <br>
						 u"\U0001F680-\U0001F6FF"  # transport & map symbols <br>
						 u"\U0001F1E0-\U0001F1FF"  # flags (iOS) <br>
						 u"\U00002702-\U000027B0" <br>
						 u"\U000024C2-\U0001F251" <br>
						 "]+", flags=re.UNICODE) <br>

					# combine sad and happy emoticons <br>
					emoticons = emoticons_happy.union(emoticons_sad) <br>

					stop_words = set(stopwords.words('english')) <br>
					word_tokens = nltk.word_tokenize(tweet) <br>

					tweet = re.sub(r':', '', tweet) <br>
					tweet = re.sub(r'‚Ä¶', '', tweet) <br>

					#replace consecutive non-ASCII characters with a space <br>
					tweet = re.sub(r'[^\x00-\x7F]+',' ', tweet) <br>

					#remove emojis from tweet <br>
					tweet = emoji_pattern.sub(r'', tweet) <br>

					#filter using NLTK library append it to a string <br>
					filtered_tweet = [w for w in word_tokens if not w in stop_words] <br>

					# looping through conditions <br>
					filtered_tweet = [] <br>
					for w in word_tokens: <br>
					    # check tokens againts stop words, emoticons and punctuations <br>
					    if w not in stop_words and w not in emoticons and w not in string.punctuation: <br>
						filtered_tweet.append(w) <br>

					return ' '.join(filtered_tweet) <br>	
				</code>
				<h3>Data Analysis and Visualizations</h3>
				<p>
				Data analysis helped in getting some insights about the trends in the datasets obtained. For both influential twitter users 
				and government officials, insights on reach score, popularity score and relevance score could be attained.
				For example, finding popularity score, I examined the favorite count and retweet count for a particular twitter user,
				and grouping the two attributes resulted to a popularity score for a particular score. <br>
				<code>
					# Creating a sample dataframe containing the columns : favorite_count, retweet_count and original_author <br>
					popularity_df = df_influencer[['original_author','favorite_count','retweet_count']] <br>

					# Grouping the columns using original author and finding summation <br>
					popularity_df = popularity_df.groupby(['original_author'])['favorite_count', 'retweet_count'].sum() <br>

					# Finding a total of favorite and retweet counts per author <br>
					popularity_col = popularity_df['favorite_count'] + popularity_df['retweet_count'] <br>

					# Creating a column for the summation of favorite and retweet counts per author. <br>
					popularity_df["popularity_col"] = popularity_col <br>

					# Sorting the dataframe with the popularity column <br>
					pop_score_twitter_influencers = popularity_df.sort_values("popularity_col", ascending=False) <br>
				</code>
				
				Visualizations below shows comparisons of the metric on both government officials and influencial twitter users.
				</p>
				<h4>Popularity score for Africa goverment officials and Africa twitter influencers</h4>
				<img src="./popularity_score_gov.png" alt="popularity score government" width="300" height="300">
				<img src="./popularity_score_inf.png" alt="popularity score influencers" width="300" height="300">
				
				<p>
				For the reach score metric, a combination for both followers and followng attributes for a single user was a examined. <br>
				</p>
				<code>
				# Creating a sample dataframe containing the columns : followers, following and original_author <br>
				reach_df = df_influencer[['original_author','followers','following']] <br>

				# Grouping the columns using original author and finding summation <br>
				reach_df = reach_df.groupby(['original_author'])['followers', 'following'].sum() <br>

				# Finding a total of followers and following counts per author <br>
				reach_col = reach_df['followers'] + reach_df['following'] <br>

				# Creating a column for the summation of followers and following counts per author. <br>
				reach_df["reach_col"] = reach_col <br>

				# Sorting the dataframe with the popularity column <br>
				reach_score_twitter_influencers = reach_df.sort_values("reach_col", ascending=False)<br>
				</code>
				
				Visualizations for the reach score metric
				<h4>Reach score for Africa goverment officials and Africa twitter influencers</h4>
				<img src="./reach_score_gov.png" alt="reach score government" width="300" height="300">
				<img src="./reach_score_inf.png" alt="reach score influencers" width="300" height="300">
				
				<p>
				Relevance score was determined by the number of mentions a user gets in other tweets. <br>
				
				</p>
				<code>
					relevance_df = df_influencer[['original_author','user_mentions']] <br>
					relevance_df = relevance_df.groupby('original_author').count() <br>
					rel_score_twitter_influencers = relevance_df.sort_values('user_mentions', ascending=False) <br>
					rel_score_twitter_influencers = rel_score_twitter_influencers .reset_index() <br>
					rel_score_twitter_influencers <br>
				</code>
				<p>The visualization for this metric is displayed below</p>
					
				<h4>Relevance score for Africa goverment officials and Africa twitter influencers</h4>
				<img src="./relevance_score_gov.png" alt="relevance score government" width="300" height="300">
				<img src="./relevance_score_inf.png" alt="relevance score influencers" width="300" height="300">
			</p>

			<h3>Conclusion and Recommendation</h3>
			<p>
				Retweet influence, mentions influence and indegree influence have big a role to play in determining the
			level of influence a user has on the society and its environment. And this is a strength that can be
			tapped into and utilized by businesses to push products and services through campaigns by partnering
			with the most influential users through marketing and advertising. This is dependent on the category of
			the content of the user.
			I would recommend other factors that determine a user’s influence to be factored in determining if one
			is more influential other than the use of the three ranks of measuring influence. For instance, the
			interpersonal relationship of a user with the virtual community can be considered. 
			</p>
		</div>
	</section>
	<footer>
		<small>
			<center>
				<p><strong>CONTACTS</strong></p>
				<p>Email: brianodhiambo530@gmail.com</a></p>
				<p>Github link: <a href="https://github.com/BrianOdhiambo/">github.com/brianodhiambo</a></p>
				<p>Linkedin: <a href="https://www.linkedin.com/in/brian-odhiambo-6b036b181/">brian-odhiambo</a></p>
			</center>
		</small>
	</footer>
</body>

</html>
